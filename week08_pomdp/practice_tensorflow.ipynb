{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division",
    "\n",
    "from IPython.core import display",
    "\n",
    "import matplotlib.pyplot as plt",
    "\n",
    "%matplotlib inline",
    "\n",
    "import numpy as np",
    "\n",
    "\n",
    "# If you are running on a server, launch xvfb to record game videos",
    "\n",
    "# Please make sure you have xvfb installed",
    "\n",
    "import os",
    "\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:",
    "\n",
    "    !bash ../xvfb start",
    "\n",
    "    %env DISPLAY = : 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KungFuMaster env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym",
    "\n",
    "from atari_util import PreprocessAtari",
    "\n",
    "\n",
    "\n",
    "def make_env():",
    "\n",
    "    env = gym.make(\"KungFuMasterDeterministic-v0\")",
    "\n",
    "    env = PreprocessAtari(env, height=42, width=42,",
    "\n",
    "                          crop=lambda img: img[60:-30, 15:],",
    "\n",
    "                          dim_order='tensorflow',",
    "\n",
    "                          color=False, n_frames=1)",
    "\n",
    "    return env",
    "\n",
    "\n",
    "\n",
    "env = make_env()",
    "\n",
    "\n",
    "obs_shape = env.observation_space.shape",
    "\n",
    "n_actions = env.action_space.n",
    "\n",
    "\n",
    "print(\"Observation shape:\", obs_shape)",
    "\n",
    "print(\"Num actions:\", n_actions)",
    "\n",
    "print(\"Action names:\", env.env.env.get_action_meanings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = env.reset()",
    "\n",
    "for _ in range(100):",
    "\n",
    "    s, _, _, _ = env.step(env.action_space.sample())",
    "\n",
    "\n",
    "plt.title('Game image')",
    "\n",
    "plt.imshow(env.render('rgb_array'))",
    "\n",
    "plt.show()",
    "\n",
    "\n",
    "plt.title('Agent observation')",
    "\n",
    "plt.imshow(s.reshape([42, 42]))",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple agent for fully-observable MDP\n",
    "\n",
    "Here's a code for an agent that only uses feedforward layers. Please read it carefully: you'll have to extend it later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf",
    "\n",
    "tf.reset_default_graph()",
    "\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, Dense, Flatten",
    "\n",
    "\n",
    "\n",
    "class FeedforwardAgent:",
    "\n",
    "    def __init__(self, name, obs_shape, n_actions, reuse=False):",
    "\n",
    "        \"\"\"A simple actor-critic agent\"\"\"",
    "\n",
    "\n",
    "        with tf.variable_scope(name, reuse=reuse):",
    "\n",
    "            # Note: number of units/filters is arbitrary, you can and should change it at your will",
    "\n",
    "            self.conv0 = Conv2D(32, (3, 3), strides=(2, 2), activation='elu')",
    "\n",
    "            self.conv1 = Conv2D(32, (3, 3), strides=(2, 2), activation='elu')",
    "\n",
    "            self.conv2 = Conv2D(32, (3, 3), strides=(2, 2), activation='elu')",
    "\n",
    "            self.flatten = Flatten()",
    "\n",
    "            self.hid = Dense(128, activation='elu')",
    "\n",
    "            self.logits = Dense(n_actions)",
    "\n",
    "            self.state_value = Dense(1)",
    "\n",
    "\n",
    "            # prepare a graph for agent step",
    "\n",
    "            _initial_state = self.get_initial_state(1)",
    "\n",
    "            self.prev_state_placeholders = [tf.placeholder(m.dtype,",
    "\n",
    "                                                           [None] + [m.shape[i] for i in range(1, m.ndim)])",
    "\n",
    "                                            for m in _initial_state]",
    "\n",
    "            self.obs_t = tf.placeholder('float32', [None, ] + list(obs_shape))",
    "\n",
    "            self.next_state, self.agent_outputs = self.symbolic_step(",
    "\n",
    "                self.prev_state_placeholders, self.obs_t)",
    "\n",
    "\n",
    "    def symbolic_step(self, prev_state, obs_t):",
    "\n",
    "        \"\"\"Takes agent's previous step and observation, returns next state and whatever it needs to learn (tf tensors)\"\"\"",
    "\n",
    "\n",
    "        nn = self.conv0(obs_t)",
    "\n",
    "        nn = self.conv1(nn)",
    "\n",
    "        nn = self.conv2(nn)",
    "\n",
    "        nn = self.flatten(nn)",
    "\n",
    "        nn = self.hid(nn)",
    "\n",
    "        logits = self.logits(nn)",
    "\n",
    "        state_value = self.state_value(nn)",
    "\n",
    "\n",
    "        # feedforward agent has no state",
    "\n",
    "        new_state = []",
    "\n",
    "\n",
    "        return new_state, (logits, state_value)",
    "\n",
    "\n",
    "    def get_initial_state(self, batch_size):",
    "\n",
    "        \"\"\"Return a list of agent memory states at game start. Each state is a np array of shape [batch_size, ...]\"\"\"",
    "\n",
    "        # feedforward agent has no state",
    "\n",
    "        return []",
    "\n",
    "\n",
    "    def step(self, prev_state, obs_t):",
    "\n",
    "        \"\"\"Same as symbolic state except it operates on numpy arrays\"\"\"",
    "\n",
    "        sess = tf.get_default_session()",
    "\n",
    "        feed_dict = {self.obs_t: obs_t}",
    "\n",
    "        for state_ph, state_value in zip(self.prev_state_placeholders, prev_state):",
    "\n",
    "            feed_dict[state_ph] = state_value",
    "\n",
    "        return sess.run([self.next_state, self.agent_outputs], feed_dict)",
    "\n",
    "\n",
    "    def sample_actions(self, agent_outputs):",
    "\n",
    "        \"\"\"pick actions given numeric agent outputs (np arrays)\"\"\"",
    "\n",
    "        logits, state_values = agent_outputs",
    "\n",
    "        policy = np.exp(logits) / np.sum(np.exp(logits),",
    "\n",
    "                                         axis=-1, keepdims=True)",
    "\n",
    "        return [np.random.choice(len(p), p=p) for p in policy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_parallel_games = 5",
    "\n",
    "gamma = 0.99",
    "\n",
    "\n",
    "agent = FeedforwardAgent(\"agent\", obs_shape, n_actions)",
    "\n",
    "\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = [env.reset()]",
    "\n",
    "_, (logits, value) = agent.step(agent.get_initial_state(1), state)",
    "\n",
    "print(\"action logits:\\n\", logits)",
    "\n",
    "print(\"state values:\\n\", value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's play!\n",
    "Let's build a function that measures agent's average reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(agent, env, n_games=1):",
    "\n",
    "    \"\"\"Plays an entire game start to end, returns session rewards.\"\"\"",
    "\n",
    "\n",
    "    game_rewards = []",
    "\n",
    "    for _ in range(n_games):",
    "\n",
    "        # initial observation and memory",
    "\n",
    "        observation = env.reset()",
    "\n",
    "        prev_memories = agent.get_initial_state(1)",
    "\n",
    "\n",
    "        total_reward = 0",
    "\n",
    "        while True:",
    "\n",
    "            new_memories, readouts = agent.step(",
    "\n",
    "                prev_memories, observation[None, ...])",
    "\n",
    "            action = agent.sample_actions(readouts)",
    "\n",
    "\n",
    "            observation, reward, done, info = env.step(action[0])",
    "\n",
    "\n",
    "            total_reward += reward",
    "\n",
    "            prev_memories = new_memories",
    "\n",
    "            if done:",
    "\n",
    "                break",
    "\n",
    "\n",
    "        game_rewards.append(total_reward)",
    "\n",
    "    return game_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_monitor = gym.wrappers.Monitor(env, directory=\"kungfu_videos\", force=True)",
    "\n",
    "rw = evaluate(agent, env_monitor, n_games=3,)",
    "\n",
    "env_monitor.close()",
    "\n",
    "print(rw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show video",
    "\n",
    "from IPython.display import HTML",
    "\n",
    "import os",
    "\n",
    "\n",
    "video_names = list(filter(lambda s: s.endswith(",
    "\n",
    "    \".mp4\"), os.listdir(\"./kungfu_videos/\")))",
    "\n",
    "\n",
    "HTML(\"\"\"",
    "\n",
    "<video width=\"640\" height=\"480\" controls>",
    "\n",
    "  <source src=\"{}\" type=\"video/mp4\">",
    "\n",
    "</video>",
    "\n",
    "\"\"\".format(\"./kungfu_videos/\"+video_names[-1]))  # this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on parallel games\n",
    "\n",
    "We introduce a class called EnvPool - it's a tool that handles multiple environments for you. Here's how it works:\n",
    "![img](https://s7.postimg.org/4y36s2b2z/env_pool.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from env_pool import EnvPool",
    "\n",
    "pool = EnvPool(agent, make_env, n_parallel_games)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each of n_parallel_games, take 10 steps",
    "\n",
    "rollout_obs, rollout_actions, rollout_rewards, rollout_mask = pool.interact(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Actions shape:\", rollout_actions.shape)",
    "\n",
    "print(\"Rewards shape:\", rollout_rewards.shape)",
    "\n",
    "print(\"Mask shape:\", rollout_mask.shape)",
    "\n",
    "print(\"Observations shape: \", rollout_obs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor-critic\n",
    "\n",
    "Here we define a loss function that uses rollout above to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observations_ph = tf.placeholder('float32', [None, None, ] + list(obs_shape))",
    "\n",
    "actions_ph = tf.placeholder('int32', (None, None,))",
    "\n",
    "rewards_ph = tf.placeholder('float32', (None, None,))",
    "\n",
    "mask_ph = tf.placeholder('float32', (None, None,))",
    "\n",
    "\n",
    "initial_memory_ph = agent.prev_state_placeholders",
    "\n",
    "dummy_outputs = agent.symbolic_step(",
    "\n",
    "    initial_memory_ph, observations_ph[:, 0])[1]",
    "\n",
    "\n",
    "_, outputs_seq = tf.scan(",
    "\n",
    "    lambda stack, obs_t: agent.symbolic_step(stack[0], obs_t),",
    "\n",
    "    initializer=(initial_memory_ph, dummy_outputs),",
    "\n",
    "    # [time, batch, h, w, c]",
    "\n",
    "    elems=tf.transpose(observations_ph, [1, 0, 2, 3, 4])",
    "\n",
    ")",
    "\n",
    "\n",
    "# from [time, batch] back to [batch, time]",
    "\n",
    "outputs_seq = [tf.transpose(",
    "\n",
    "    tensor, [1, 0] + list(range(2, tensor.shape.ndims))) for tensor in outputs_seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actor-critic losses",
    "\n",
    "# logits shape: [batch, time, n_actions], states shape: [batch, time, n_actions]",
    "\n",
    "logits_seq, state_values_seq = outputs_seq",
    "\n",
    "\n",
    "logprobs_seq = tf.nn.log_softmax(logits_seq)",
    "\n",
    "logp_actions = tf.reduce_sum(",
    "\n",
    "    logprobs_seq * tf.one_hot(actions_ph, n_actions), axis=-1)[:, :-1]",
    "\n",
    "\n",
    "current_rewards = rewards_ph[:, :-1] / 100.",
    "\n",
    "current_state_values = state_values_seq[:, :-1, 0]",
    "\n",
    "next_state_values = state_values_seq[:, 1:, 0] * mask_ph[:, :-1]",
    "\n",
    "\n",
    "# policy gradient",
    "\n",
    "# compute 1-step advantage using current_rewards, current_state_values and next_state_values",
    "\n",
    "advantage =  # YOUR CODE",
    "\n",
    "assert advantage.shape.ndims == 2",
    "\n",
    "# compute policy entropy given logits_seq. Mind the sign!",
    "\n",
    "entropy =  # YOUR CODE",
    "\n",
    "assert entropy.shape.ndims == 2",
    "\n",
    "\n",
    "actor_loss = - tf.reduce_mean(logp_actions *",
    "\n",
    "                              tf.stop_gradient(advantage)) - 1e-2 * tf.reduce_mean(entropy)",
    "\n",
    "\n",
    "# compute target qvalues using temporal difference",
    "\n",
    "target_qvalues =  # YOUR CODE",
    "\n",
    "critic_loss = tf.reduce_mean(",
    "\n",
    "    (current_state_values - tf.stop_gradient(target_qvalues))**2)",
    "\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(1e-5).minimize(actor_loss + critic_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train \n",
    "\n",
    "just run train step and see if agent learns any better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_batch(rollout_length=10):",
    "\n",
    "    prev_mem = pool.prev_memory_states",
    "\n",
    "    rollout_obs, rollout_actions, rollout_rewards, rollout_mask = pool.interact(",
    "\n",
    "        rollout_length)",
    "\n",
    "\n",
    "    feed_dict = {",
    "\n",
    "        observations_ph: rollout_obs,",
    "\n",
    "        actions_ph: rollout_actions,",
    "\n",
    "        rewards_ph: rollout_rewards,",
    "\n",
    "        mask_ph: rollout_mask,",
    "\n",
    "    }",
    "\n",
    "    for placeholder, value in zip(initial_memory_ph, prev_mem):",
    "\n",
    "        feed_dict[placeholder] = value",
    "\n",
    "    return feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output",
    "\n",
    "from tqdm import trange",
    "\n",
    "from pandas import DataFrame",
    "\n",
    "moving_average = lambda x, **kw: DataFrame(",
    "\n",
    "    {'x': np.asarray(x)}).x.ewm(**kw).mean().values",
    "\n",
    "\n",
    "rewards_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in trange(5000):",
    "\n",
    "    sess.run(train_step, sample_batch())",
    "\n",
    "\n",
    "    if i % 100 == 0:",
    "\n",
    "        rewards_history.append(np.mean(evaluate(agent, env, n_games=1)))",
    "\n",
    "        clear_output(True)",
    "\n",
    "        plt.plot(rewards_history, label='rewards')",
    "\n",
    "        plt.plot(moving_average(np.array(rewards_history),",
    "\n",
    "                                span=10), label='rewards ewma@10')",
    "\n",
    "        plt.legend()",
    "\n",
    "        plt.show()",
    "\n",
    "        if rewards_history[-1] >= 10000:",
    "\n",
    "            print(\"Your agent has just passed the minimum homework threshold\")",
    "\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Final\" evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_monitor = gym.wrappers.Monitor(env, directory=\"kungfu_videos\", force=True)",
    "\n",
    "final_rewards = evaluate(agent, env_monitor, n_games=20,)",
    "\n",
    "env_monitor.close()",
    "\n",
    "print(\"Final mean reward\": np.mean(final_rewards))",
    "\n",
    "\n",
    "video_names = list(filter(lambda s: s.endswith(",
    "\n",
    "    \".mp4\"), os.listdir(\"./kungfu_videos/\")))",
    "\n",
    "HTML(\"\"\"",
    "\n",
    "<video width=\"640\" height=\"480\" controls>",
    "\n",
    "  <source src=\"{}\" type=\"video/mp4\">",
    "\n",
    "</video>",
    "\n",
    "\"\"\".format(\"./kungfu_videos/\"+video_names[-1]))  # this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POMDP setting\n",
    "\n",
    "The atari game we're working with is actually a POMDP: your agent needs to know timing at which enemies spawn and move, but cannot do so unless it has some memory. \n",
    "\n",
    "Let's design another agent that has a recurrent neural net memory to solve this.\n",
    "\n",
    "__Note:__ it's also a good idea to scale rollout_len up to learn longer sequences. You may wish set it to >=20 or to start at 10 and then scale up as time passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRecurrentAgent:",
    "\n",
    "    def __init__(self, name, obs_shape, n_actions, reuse=False):",
    "\n",
    "        \"\"\"A simple actor-critic agent\"\"\"",
    "\n",
    "\n",
    "        with tf.variable_scope(name, reuse=reuse):",
    "\n",
    "            # Note: number of units/filters is arbitrary, you can and should change it at your will",
    "\n",
    "            self.conv0 = Conv2D(32, (3, 3), strides=(2, 2), activation='elu')",
    "\n",
    "            self.conv1 = Conv2D(32, (3, 3), strides=(2, 2), activation='elu')",
    "\n",
    "            self.conv2 = Conv2D(32, (3, 3), strides=(2, 2), activation='elu')",
    "\n",
    "            self.flatten = Flatten()",
    "\n",
    "            self.hid = Dense(128, activation='elu')",
    "\n",
    "\n",
    "            self.rnn0 = tf.nn.rnn_cell.GRUCell(256, activation=tf.tanh)",
    "\n",
    "\n",
    "            self.logits = Dense(n_actions)",
    "\n",
    "            self.state_value = Dense(1)",
    "\n",
    "\n",
    "            # prepare a graph for agent step",
    "\n",
    "            _initial_state = self.get_initial_state(1)",
    "\n",
    "            self.prev_state_placeholders = [tf.placeholder(m.dtype,",
    "\n",
    "                                                           [None] + [m.shape[i] for i in range(1, m.ndim)])",
    "\n",
    "                                            for m in _initial_state]",
    "\n",
    "            self.obs_t = tf.placeholder('float32', [None, ] + list(obs_shape))",
    "\n",
    "            self.next_state, self.agent_outputs = self.symbolic_step(",
    "\n",
    "                self.prev_state_placeholders, self.obs_t)",
    "\n",
    "\n",
    "    def symbolic_step(self, prev_state, obs_t):",
    "\n",
    "        \"\"\"Takes agent's previous step and observation, returns next state and whatever it needs to learn (tf tensors)\"\"\"",
    "\n",
    "\n",
    "        nn = self.conv0(obs_t)",
    "\n",
    "        nn = self.conv1(nn)",
    "\n",
    "        nn = self.conv2(nn)",
    "\n",
    "        nn = self.flatten(nn)",
    "\n",
    "        nn = self.hid(nn)",
    "\n",
    "\n",
    "        (prev_rnn0,) = prev_state",
    "\n",
    "\n",
    "        # YOUR CODE: apply recurrent neural net for one step here.",
    "\n",
    "        # See docs on self.rnn0(...)",
    "\n",
    "        # the recurrent cell should take the last feedforward dense layer as input",
    "\n",
    "        raise NotImplementedError(\"Please implement rnn step\")",
    "\n",
    "\n",
    "        logits = self.logits( < some layer > )",
    "\n",
    "        state_value = self.state_value( < some layer > )",
    "\n",
    "\n",
    "        new_state = [new_rnn0]",
    "\n",
    "\n",
    "        return new_state, (logits, state_value)",
    "\n",
    "\n",
    "    def get_initial_state(self, batch_size):",
    "\n",
    "        \"\"\"Return a list of agent memory states at game start. Each state is a np array of shape [batch_size, ...]\"\"\"",
    "\n",
    "        # feedforward agent has no state",
    "\n",
    "        return [np.zeros([batch_size, self.rnn0.output_size], 'float32')]",
    "\n",
    "\n",
    "    def step(self, prev_state, obs_t):",
    "\n",
    "        \"\"\"Same as symbolic state except it operates on numpy arrays\"\"\"",
    "\n",
    "        sess = tf.get_default_session()",
    "\n",
    "        feed_dict = {self.obs_t: obs_t}",
    "\n",
    "        for state_ph, state_value in zip(self.prev_state_placeholders, prev_state):",
    "\n",
    "            feed_dict[state_ph] = state_value",
    "\n",
    "        return sess.run([self.next_state, self.agent_outputs], feed_dict)",
    "\n",
    "\n",
    "    def sample_actions(self, agent_outputs):",
    "\n",
    "        \"\"\"pick actions given numeric agent outputs (np arrays)\"\"\"",
    "\n",
    "        logits, state_values = agent_outputs",
    "\n",
    "        policy = np.exp(logits) / np.sum(np.exp(logits),",
    "\n",
    "                                         axis=-1, keepdims=True)",
    "\n",
    "        return [np.random.choice(len(p), p=p) for p in policy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = SimpleRecurrentAgent('agent_with_memory', obs_shape, n_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's train it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A whole lot of your code here: train the new agent with GRU memory.",
    "\n",
    "# - create pool",
    "\n",
    "# - write loss functions and training op",
    "\n",
    "# - train",
    "\n",
    "# You can reuse most of the code with zero to few changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "```\n",
    "```\n",
    "\n",
    "```\n",
    "```\n",
    "\n",
    "```\n",
    "```\n",
    "\n",
    "```\n",
    "```\n",
    "\n",
    "```\n",
    "```\n",
    "\n",
    "```\n",
    "```\n",
    "\n",
    "```\n",
    "```\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework assignment is in the second notebook: [url]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
