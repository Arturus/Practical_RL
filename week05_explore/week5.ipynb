{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this if in COLAB\n",
    "!pip install --upgrade https://github.com/Theano/Theano/archive/master.zip\n",
    "!pip install --upgrade https://github.com/Lasagne/Lasagne/archive/master.zip\n",
    "    \n",
    "!wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week5_explore/bayes.py\n",
    "!wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week5_explore/action_rewards.npy\n",
    "!wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week5_explore/all_states.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABCMeta, abstractmethod, abstractproperty\n",
    "import enum\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=3)\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "import pandas\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "* [1. Bernoulli Bandit](#Part-1.-Bernoulli-Bandit)\n",
    "    * [Bonus 1.1. Gittins index (5 points)](#Bonus-1.1.-Gittins-index-%285-points%29.)\n",
    "    * [HW 1.1. Nonstationary Bernoulli bandit](#HW-1.1.-Nonstationary-Bernoulli-bandit)\n",
    "* [2. Contextual bandit](#Part-2.-Contextual-bandit)\n",
    "    * [2.1 Bulding a BNN agent](#2.1-Bulding-a-BNN-agent)\n",
    "    * [2.2 Training the agent](#2.2-Training-the-agent)\n",
    "    * [HW 2.1 Better exploration](#HW-2.1-Better-exploration)\n",
    "* [3. Exploration in MDP](#Part-3.-Exploration-in-MDP)\n",
    "    * [Bonus 3.1 Posterior sampling RL (3 points)](#Bonus-3.1-Posterior-sampling-RL-%283-points%29)\n",
    "    * [Bonus 3.2 Bootstrapped DQN (10 points)](#Bonus-3.2-Bootstrapped-DQN-%2810-points%29)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Bernoulli Bandit\n",
    "\n",
    "We are going to implement several exploration strategies for simplest problem - bernoulli bandit.\n",
    "\n",
    "The bandit has $K$ actions. Action produce 1.0 reward $r$ with probability $0 \\le \\theta_k \\le 1$ which is unknown to agent, but fixed over time. Agent's objective is to minimize regret over fixed number $T$ of action selections:\n",
    "\n",
    "$$\\rho = T\\theta^* - \\sum_{t=1}^T r_t$$\n",
    "\n",
    "Where $\\theta^* = \\max_k\\{\\theta_k\\}$\n",
    "\n",
    "**Real-world analogy:**\n",
    "\n",
    "Clinical trials - we have $K$ pills and $T$ ill patient. After taking pill, patient is cured with probability $\\theta_k$. Task is to find most efficient pill.\n",
    "\n",
    "A research on clinical trials - https://arxiv.org/pdf/1507.08025.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BernoulliBandit:\n",
    "    def __init__(self, n_actions=5):\n",
    "        self._probs = np.random.random(n_actions)\n",
    "\n",
    "    @property\n",
    "    def action_count(self):\n",
    "        return len(self._probs)\n",
    "\n",
    "    def pull(self, action):\n",
    "        if np.random.random() > self._probs[action]:\n",
    "            return 0.0\n",
    "        return 1.0\n",
    "\n",
    "    def optimal_reward(self):\n",
    "        \"\"\" Used for regret calculation\n",
    "        \"\"\"\n",
    "        return np.max(self._probs)\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\" Used in nonstationary version\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\" Used in nonstationary version\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbstractAgent(metaclass=ABCMeta):\n",
    "    def init_actions(self, n_actions):\n",
    "        self._successes = np.zeros(n_actions)\n",
    "        self._failures = np.zeros(n_actions)\n",
    "        self._total_pulls = 0\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_action(self):\n",
    "        \"\"\"\n",
    "        Get current best action\n",
    "        :rtype: int\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def update(self, action, reward):\n",
    "        \"\"\"\n",
    "        Observe reward from action and update agent's internal parameters\n",
    "        :type action: int\n",
    "        :type reward: int\n",
    "        \"\"\"\n",
    "        self._total_pulls += 1\n",
    "        if reward == 1:\n",
    "            self._successes[action] += 1\n",
    "        else:\n",
    "            self._failures[action] += 1\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return self.__class__.__name__\n",
    "\n",
    "\n",
    "class RandomAgent(AbstractAgent):\n",
    "    def get_action(self):\n",
    "        return np.random.randint(0, len(self._successes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epsilon-greedy agent\n",
    "\n",
    "**for** $t = 1,2,...$ **do**\n",
    "\n",
    "&nbsp;&nbsp; **for** $k = 1,...,K$ **do**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\hat\\theta_k \\leftarrow \\alpha_k / (\\alpha_k + \\beta_k)$\n",
    "\n",
    "&nbsp;&nbsp; **end for** \n",
    "\n",
    "&nbsp;&nbsp; $x_t \\leftarrow argmax_{k}\\hat\\theta$ with probability $1 - \\epsilon$ or random action with probability $\\epsilon$\n",
    "\n",
    "&nbsp;&nbsp; Apply $x_t$ and observe $r_t$\n",
    "\n",
    "&nbsp;&nbsp; $(\\alpha_{x_t}, \\beta_{x_t}) \\leftarrow (\\alpha_{x_t}, \\beta_{x_t}) + (r_t, 1-r_t)$\n",
    "\n",
    "**end for**\n",
    "\n",
    "Implement the algorithm above in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyAgent(AbstractAgent):\n",
    "    def __init__(self, epsilon=0.01):\n",
    "        self._epsilon = epsilon\n",
    "\n",
    "    def get_action(self):\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return self.__class__.__name__ + \"(epsilon={})\".format(self._epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UCB Agent\n",
    "Epsilon-greedy strategy heve no preference for actions. It would be better to select among actions that are uncertain or have potential to be optimal. One can come up with idea of index for each action that represents otimality and uncertainty at the same time. One efficient way to do it is to use UCB1 algorithm:\n",
    "\n",
    "**for** $t = 1,2,...$ **do**\n",
    "\n",
    "&nbsp;&nbsp; **for** $k = 1,...,K$ **do**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $w_k \\leftarrow \\alpha_k / (\\alpha_k + \\beta_k) + \\sqrt{2log\\ t \\ / \\ (\\alpha_k + \\beta_k)}$\n",
    "\n",
    "&nbsp;&nbsp; **end for** \n",
    "\n",
    "&nbsp;&nbsp; **end for** \n",
    " $x_t \\leftarrow argmax_{k}w$\n",
    "\n",
    "&nbsp;&nbsp; Apply $x_t$ and observe $r_t$\n",
    "\n",
    "&nbsp;&nbsp; $(\\alpha_{x_t}, \\beta_{x_t}) \\leftarrow (\\alpha_{x_t}, \\beta_{x_t}) + (r_t, 1-r_t)$\n",
    "\n",
    "**end for**\n",
    "\n",
    "More versions and optimality analysis - https://homes.di.unimi.it/~cesabian/Pubblicazioni/ml-02.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCBAgent(AbstractAgent):\n",
    "    def get_action(self):\n",
    "        # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thompson sampling\n",
    "\n",
    "UCB1 algorithm does not take into account actual distribution of rewards. If we know the distribution - we can do much better by using Thompson sampling:\n",
    "\n",
    "**for** $t = 1,2,...$ **do**\n",
    "\n",
    "&nbsp;&nbsp; **for** $k = 1,...,K$ **do**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Sample $\\hat\\theta_k \\sim beta(\\alpha_k, \\beta_k)$\n",
    "\n",
    "&nbsp;&nbsp; **end for** \n",
    "\n",
    "&nbsp;&nbsp; $x_t \\leftarrow argmax_{k}\\hat\\theta$\n",
    "\n",
    "&nbsp;&nbsp; Apply $x_t$ and observe $r_t$\n",
    "\n",
    "&nbsp;&nbsp; $(\\alpha_{x_t}, \\beta_{x_t}) \\leftarrow (\\alpha_{x_t}, \\beta_{x_t}) + (r_t, 1-r_t)$\n",
    "\n",
    "**end for**\n",
    " \n",
    "\n",
    "More on Thompson Sampling:\n",
    "https://web.stanford.edu/~bvr/pubs/TS_Tutorial.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThompsonSamplingAgent(AbstractAgent):\n",
    "    def get_action(self):\n",
    "        # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_regret(env, agents, n_steps=5000, n_trials=50):\n",
    "    scores = {\n",
    "        agent.name: [0.0 for step in range(n_steps)] for agent in agents\n",
    "    }\n",
    "\n",
    "    for trial in range(n_trials):\n",
    "        env.reset()\n",
    "\n",
    "        for a in agents:\n",
    "            a.init_actions(env.action_count)\n",
    "\n",
    "        for i in range(n_steps):\n",
    "            optimal_reward = env.optimal_reward()\n",
    "\n",
    "            for agent in agents:\n",
    "                action = agent.get_action()\n",
    "                reward = env.pull(action)\n",
    "                agent.update(action, reward)\n",
    "                scores[agent.name][i] += optimal_reward - reward\n",
    "\n",
    "            env.step()  # change bandit's state if it is unstationary\n",
    "\n",
    "    plt.figure(figsize=(17, 8))\n",
    "    for agent in agents:\n",
    "        plt.plot(np.cumsum(scores[agent.name]) / n_trials)\n",
    "\n",
    "    plt.legend([agent.name for agent in agents])\n",
    "\n",
    "    plt.ylabel(\"regret\")\n",
    "    plt.xlabel(\"steps\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment agents\n",
    "agents = [\n",
    "    #     EpsilonGreedyAgent(),\n",
    "    #     UCBAgent(),\n",
    "    #     ThompsonSamplingAgent()\n",
    "]\n",
    "\n",
    "plot_regret(BernoulliBandit(), agents, n_steps=10000, n_trials=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus 1.1. Gittins index (5 points).\n",
    "\n",
    "Bernoulli bandit problem has an optimal solution - Gittins index algorithm. Implement finite horizon version of the algorithm and demonstrate it's performance with experiments. some articles:\n",
    "- Wikipedia article - https://en.wikipedia.org/wiki/Gittins_index\n",
    "- Different algorithms for index computation - http://www.ece.mcgill.ca/~amahaj1/projects/bandits/book/2013-bandit-computations.pdf (see \"Bernoulli\" section)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 1.1. Nonstationary Bernoulli bandit\n",
    "\n",
    "What if success probabilities change over time? Here is an example of such bandit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DriftingBandit(BernoulliBandit):\n",
    "    def __init__(self, n_actions=5, gamma=0.01):\n",
    "        \"\"\"\n",
    "        Idea from https://github.com/iosband/ts_tutorial\n",
    "        \"\"\"\n",
    "        super().__init__(n_actions)\n",
    "\n",
    "        self._gamma = gamma\n",
    "\n",
    "        self._successes = None\n",
    "        self._failures = None\n",
    "        self._steps = 0\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self._successes = np.zeros(self.action_count) + 1.0\n",
    "        self._failures = np.zeros(self.action_count) + 1.0\n",
    "        self._steps = 0\n",
    "\n",
    "    def step(self):\n",
    "        action = np.random.randint(self.action_count)\n",
    "        reward = self.pull(action)\n",
    "        self._step(action, reward)\n",
    "\n",
    "    def _step(self, action, reward):\n",
    "        self._successes = self._successes * (1 - self._gamma) + self._gamma\n",
    "        self._failures = self._failures * (1 - self._gamma) + self._gamma\n",
    "        self._steps += 1\n",
    "\n",
    "        self._successes[action] += reward\n",
    "        self._failures[action] += 1.0 - reward\n",
    "\n",
    "        self._probs = np.random.beta(self._successes, self._failures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a picture how it's reward probabilities change over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drifting_env = DriftingBandit(n_actions=5)\n",
    "\n",
    "drifting_probs = []\n",
    "for i in range(20000):\n",
    "    drifting_env.step()\n",
    "    drifting_probs.append(drifting_env._probs)\n",
    "\n",
    "plt.figure(figsize=(17, 8))\n",
    "plt.plot(pandas.DataFrame(drifting_probs).rolling(window=20).mean())\n",
    "\n",
    "plt.xlabel(\"steps\")\n",
    "plt.ylabel(\"Success probability\")\n",
    "plt.title(\"Reward probabilities over time\")\n",
    "plt.legend([\"Action {}\".format(i) for i in range(drifting_env.action_count)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to invent an agent that will have better regret than stationary agents from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR AGENT HERE SECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drifting_agents = [\n",
    "    ThompsonSamplingAgent(),\n",
    "    EpsilonGreedyAgent(),\n",
    "    UCBAgent(),\n",
    "    YourAgent()\n",
    "]\n",
    "\n",
    "plot_regret(DriftingBandit(), drifting_agents, n_steps=20000, n_trials=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Contextual bandit\n",
    "\n",
    "Now we will solve much more complex problem - reward will depend on bandit's state.\n",
    "\n",
    "**Real-word analogy:**\n",
    "\n",
    "> Contextual advertising. We have a lot of banners and a lot of different users. Users can have different features: age, gender, search requests. We want to show banner with highest click probability.\n",
    "\n",
    "If we want use strategies from above, we need some how store reward distributions conditioned both on actions and bandit's state. \n",
    "One way to do this - use bayesian neural networks. Instead of giving pointwise estimates of target, they maintain probability distributions\n",
    "\n",
    "<img src=\"bnn.png\">\n",
    "Picture from https://arxiv.org/pdf/1505.05424.pdf\n",
    "\n",
    "\n",
    "More material:\n",
    "  * A post on the matter - [url](http://twiecki.github.io/blog/2016/07/05/bayesian-deep-learning/)\n",
    "  * Theano+PyMC3 for more serious stuff - [url](http://pymc-devs.github.io/pymc3/notebooks/bayesian_neural_network_advi.html)\n",
    "  * Same stuff in tensorflow - [url](http://edwardlib.org/tutorials/bayesian-neural-network)\n",
    "  \n",
    "Let's load our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_states = np.load(\"all_states.npy\")\n",
    "action_rewards = np.load(\"action_rewards.npy\")\n",
    "\n",
    "state_size = all_states.shape[1]\n",
    "n_actions = action_rewards.shape[1]\n",
    "\n",
    "print(\"State size: %i, actions: %i\" % (state_size, n_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "from lasagne import init\n",
    "from lasagne.layers import *\n",
    "import bayes\n",
    "\n",
    "as_bayesian = bayes.bbpwrap(bayes.NormalApproximation(std=0.1))\n",
    "BayesDenseLayer = as_bayesian(DenseLayer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Bulding a BNN agent\n",
    "\n",
    "Let's implement epsilon-greedy BNN agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BNNAgent:\n",
    "    \"\"\"a bandit with bayesian neural net\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, n_actions):\n",
    "        input_states = T.matrix(\"states\")\n",
    "        target_actions = T.ivector(\"actions taken\")\n",
    "        target_rewards = T.vector(\"rewards\")\n",
    "\n",
    "        self.total_samples_seen = theano.shared(\n",
    "            np.int32(0), \"number of training samples seen so far\")\n",
    "        batch_size = target_actions.shape[0]  # por que?\n",
    "\n",
    "        # Network\n",
    "        inp = InputLayer((None, state_size), name='input')\n",
    "        # YOUR NETWORK HERE\n",
    "        out = <Your network >\n",
    "\n",
    "        # Prediction\n",
    "        prediction_all_actions = get_output(out, inputs=input_states)\n",
    "        self.predict_sample_rewards = theano.function(\n",
    "            [input_states], prediction_all_actions)\n",
    "\n",
    "        # Training\n",
    "\n",
    "        # select prediction for target action\n",
    "        prediction_target_actions = prediction_all_actions[T.arange(\n",
    "            batch_size), target_actions]\n",
    "\n",
    "        # loss = negative log-likelihood (mse) + KL\n",
    "        negative_llh = T.sum((prediction_target_actions - target_rewards)**2)\n",
    "\n",
    "        kl = bayes.get_var_cost(out) / (self.total_samples_seen+batch_size)\n",
    "\n",
    "        loss = (negative_llh + kl)/batch_size\n",
    "\n",
    "        self.weights = get_all_params(out, trainable=True)\n",
    "        self.out = out\n",
    "\n",
    "        # gradient descent\n",
    "        updates = lasagne.updates.adam(loss, self.weights)\n",
    "        # update counts\n",
    "        updates[self.total_samples_seen] = self.total_samples_seen + \\\n",
    "            batch_size.astype('int32')\n",
    "\n",
    "        self.train_step = theano.function([input_states, target_actions, target_rewards],\n",
    "                                          [negative_llh, kl],\n",
    "                                          updates=updates,\n",
    "                                          allow_input_downcast=True)\n",
    "\n",
    "    def sample_prediction(self, states, n_samples=1):\n",
    "        \"\"\"Samples n_samples predictions for rewards,\n",
    "\n",
    "        :returns: tensor [n_samples, state_i, action_i]\n",
    "        \"\"\"\n",
    "        assert states.ndim == 2, \"states must be 2-dimensional\"\n",
    "\n",
    "        return np.stack([self.predict_sample_rewards(states) for _ in range(n_samples)])\n",
    "\n",
    "    epsilon = 0.25\n",
    "\n",
    "    def get_action(self, states):\n",
    "        \"\"\"\n",
    "        Picks action by \n",
    "        - with p=1-epsilon, taking argmax of average rewards\n",
    "        - with p=epsilon, taking random action\n",
    "        This is exactly e-greedy policy.\n",
    "        \"\"\"\n",
    "\n",
    "        reward_samples = self.sample_prediction(states, n_samples=100)\n",
    "        # ^-- samples for rewards, shape = [n_samples,n_states,n_actions]\n",
    "\n",
    "        best_actions = reward_samples.mean(axis=0).argmax(axis=-1)\n",
    "        # ^-- we take mean over samples to compute expectation, then pick best action with argmax\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        chosen_actions = <-- implement epsilon-greedy strategy - ->\n",
    "\n",
    "        return chosen_actions\n",
    "\n",
    "    def train(self, states, actions, rewards, n_iters=10):\n",
    "        \"\"\"\n",
    "        trains to predict rewards for chosen actions in given states\n",
    "        \"\"\"\n",
    "        loss_sum = kl_sum = 0\n",
    "        for _ in range(n_iters):\n",
    "            loss, kl = self.train_step(states, actions, rewards)\n",
    "            loss_sum += loss\n",
    "            kl_sum += kl\n",
    "\n",
    "        return loss_sum / n_iters, kl_sum / n_iters\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return self.__class__.__name__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Training the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ITERS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_samples(states, action_rewards, batch_size=10):\n",
    "    \"\"\"samples random minibatch, emulating new users\"\"\"\n",
    "    batch_ix = np.random.randint(0, len(states), batch_size)\n",
    "    return states[batch_ix], action_rewards[batch_ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "from pandas import DataFrame\n",
    "moving_average = lambda x, **kw: DataFrame(\n",
    "    {'x': np.asarray(x)}).x.ewm(**kw).mean().values\n",
    "\n",
    "def train_contextual_agent(agent, batch_size=10, n_iters=100):\n",
    "    rewards_history = []\n",
    "\n",
    "    for i in range(n_iters):\n",
    "        b_states, b_action_rewards = get_new_samples(\n",
    "            all_states, action_rewards, batch_size)\n",
    "        b_actions = agent.get_action(b_states)\n",
    "        b_rewards = b_action_rewards[\n",
    "            np.arange(batch_size), b_actions\n",
    "        ]\n",
    "\n",
    "        mse, kl = agent.train(b_states, b_actions, b_rewards, n_iters=100)\n",
    "\n",
    "        rewards_history.append(b_rewards.mean())\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            clear_output(True)\n",
    "            print(\"iteration #%i\\tmean reward=%.3f\\tmse=%.3f\\tkl=%.3f\" %\n",
    "                  (i, np.mean(rewards_history[-10:]), mse, kl))\n",
    "            plt.plot(rewards_history)\n",
    "            plt.plot(moving_average(np.array(rewards_history), alpha=0.1))\n",
    "            plt.title(\"Reward per epesode\")\n",
    "            plt.xlabel(\"Episode\")\n",
    "            plt.ylabel(\"Reward\")\n",
    "            plt.show()\n",
    "\n",
    "            samples = agent.sample_prediction(\n",
    "                b_states[:1], n_samples=100).T[:, 0, :]\n",
    "            for i in range(len(samples)):\n",
    "                plt.hist(samples[i], alpha=0.25, label=str(i))\n",
    "                plt.legend(loc='best')\n",
    "            print('Q(s,a) std:', ';'.join(\n",
    "                list(map('{:.3f}'.format, np.std(samples, axis=1)))))\n",
    "            print('correct', b_action_rewards[0].argmax())\n",
    "            plt.title(\"p(Q(s, a))\")\n",
    "            plt.show()\n",
    "\n",
    "    return moving_average(np.array(rewards_history), alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnn_agent = BNNAgent(state_size=state_size, n_actions=n_actions)\n",
    "greedy_agent_rewards = train_contextual_agent(\n",
    "    bnn_agent, batch_size=10, n_iters=N_ITERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 2.1 Better exploration\n",
    "\n",
    "Use strategies from first part to gain more reward in contextual setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThompsonBNNAgent(BNNAgent):\n",
    "    def get_action(self, states):\n",
    "        \"\"\"\n",
    "        picks action based by taking _one_ sample from BNN and taking action with highest sampled reward (yes, that simple)\n",
    "        This is exactly thompson sampling.\n",
    "        \"\"\"\n",
    "\n",
    "        # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thompson_agent_rewards = train_contextual_agent(ThompsonBNNAgent(state_size=state_size, n_actions=n_actions),\n",
    "                                               batch_size=10, n_iters=N_ITERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesUCBBNNAgent(BNNAgent):\n",
    "    q = 90\n",
    "\n",
    "    def get_action(self, states):\n",
    "        \"\"\"\n",
    "        Compute q-th percentile of rewards P(r|s,a) for all actions\n",
    "        Take actions that have highest percentiles.\n",
    "\n",
    "        This implements bayesian UCB strategy\n",
    "        \"\"\"\n",
    "\n",
    "        # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ucb_agent_rewards = train_contextual_agent(BayesUCBBNNAgent(state_size=state_size, n_actions=n_actions),\n",
    "                                           batch_size=10, n_iters=N_ITERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(17, 8))\n",
    "\n",
    "plt.plot(greedy_agent_rewards)\n",
    "plt.plot(thompson_agent_rewards)\n",
    "plt.plot(ucb_agent_rewards)\n",
    "\n",
    "plt.legend([\n",
    "    \"Greedy BNN\",\n",
    "    \"Thompson sampling BNN\",\n",
    "    \"UCB BNN\"\n",
    "])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3. Exploration in MDP\n",
    "\n",
    "The following problem, called \"river swim\", illustrates importance of exploration in context of mdp's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"river_swim.png\">\n",
    "\n",
    "Picture from https://arxiv.org/abs/1306.0940"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rewards and transition probabilities are unknown to an agent. Optimal policy is to swim against current, while easiest way to gain reward is to go left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RiverSwimEnv:\n",
    "    LEFT_REWARD = 5.0 / 1000\n",
    "    RIGHT_REWARD = 1.0\n",
    "\n",
    "    def __init__(self, intermediate_states_count=4, max_steps=16):\n",
    "        self._max_steps = max_steps\n",
    "        self._current_state = None\n",
    "        self._steps = None\n",
    "        self._interm_states = intermediate_states_count\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self._steps = 0\n",
    "        self._current_state = 1\n",
    "        return self._current_state, 0.0, False\n",
    "\n",
    "    @property\n",
    "    def n_actions(self):\n",
    "        return 2\n",
    "\n",
    "    @property\n",
    "    def n_states(self):\n",
    "        return 2 + self._interm_states\n",
    "\n",
    "    def _get_transition_probs(self, action):\n",
    "        if action == 0:\n",
    "            if self._current_state == 0:\n",
    "                return [0, 1.0, 0]\n",
    "            else:\n",
    "                return [1.0, 0, 0]\n",
    "\n",
    "        elif action == 1:\n",
    "            if self._current_state == 0:\n",
    "                return [0, .4, .6]\n",
    "            if self._current_state == self.n_states - 1:\n",
    "                return [.4, .6, 0]\n",
    "            else:\n",
    "                return [.05, .6, .35]\n",
    "        else:\n",
    "            raise RuntumeError(\n",
    "                \"Unknown action {}. Max action is {}\".format(action, self.n_actions))\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        :param action:\n",
    "        :type action: int\n",
    "        :return: observation, reward, is_done\n",
    "        :rtype: (int, float, bool)\n",
    "        \"\"\"\n",
    "        reward = 0.0\n",
    "\n",
    "        if self._steps >= self._max_steps:\n",
    "            return self._current_state, reward, True\n",
    "\n",
    "        transition = np.random.choice(\n",
    "            range(3), p=self._get_transition_probs(action))\n",
    "        if transition == 0:\n",
    "            self._current_state -= 1\n",
    "        elif transition == 1:\n",
    "            pass\n",
    "        else:\n",
    "            self._current_state += 1\n",
    "\n",
    "        if self._current_state == 0:\n",
    "            reward = self.LEFT_REWARD\n",
    "        elif self._current_state == self.n_states - 1:\n",
    "            reward = self.RIGHT_REWARD\n",
    "\n",
    "        self._steps += 1\n",
    "        return self._current_state, reward, False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement q-learning agent with epsilon-greedy exploration strategy and see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    def __init__(self, n_states, n_actions, lr=0.2, gamma=0.95, epsilon=0.1):\n",
    "        self._gamma = gamma\n",
    "        self._epsilon = epsilon\n",
    "        self._q_matrix = np.zeros((n_states, n_actions))\n",
    "        self._lr = lr\n",
    "\n",
    "    def get_action(self, state):\n",
    "        if np.random.random() < self._epsilon:\n",
    "            return np.random.randint(0, self._q_matrix.shape[1])\n",
    "        else:\n",
    "            return np.argmax(self._q_matrix[state])\n",
    "\n",
    "    def get_q_matrix(self):\n",
    "        \"\"\" Used for policy visualization\n",
    "        \"\"\"\n",
    "\n",
    "        return self._q_matrix\n",
    "\n",
    "    def start_episode(self):\n",
    "        \"\"\" Used in PSRL agent\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def update(self, state, action, reward, next_state):\n",
    "        # YOUR CODE HERE\n",
    "        # Finish implementation of q-learnig agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mdp_agent(agent, env, n_episodes):\n",
    "    episode_rewards = []\n",
    "\n",
    "    for ep in range(n_episodes):\n",
    "        state, ep_reward, is_done = env.reset()\n",
    "        agent.start_episode()\n",
    "        while not is_done:\n",
    "            action = agent.get_action(state)\n",
    "\n",
    "            next_state, reward, is_done = env.step(action)\n",
    "            agent.update(state, action, reward, next_state)\n",
    "\n",
    "            state = next_state\n",
    "            ep_reward += reward\n",
    "\n",
    "        episode_rewards.append(ep_reward)\n",
    "    return episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = RiverSwimEnv()\n",
    "agent = QLearningAgent(env.n_states, env.n_actions)\n",
    "rews = train_mdp_agent(agent, env, 1000)\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "plt.plot(moving_average(np.array(rews), alpha=.1))\n",
    "plt.xlabel(\"Episode count\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize our policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_policy(agent):\n",
    "    fig = plt.figure(figsize=(15, 8))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.matshow(agent.get_q_matrix().T)\n",
    "    ax.set_yticklabels(['', 'left', 'right'])\n",
    "    plt.xlabel(\"State\")\n",
    "    plt.ylabel(\"Action\")\n",
    "    plt.title(\"Values of state-action pairs\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_policy(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As your see, agent uses suboptimal policy of going left and does not explore the right state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus 3.1 Posterior sampling RL (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will implement Thompson Sampling for MDP!\n",
    "\n",
    "General algorithm:\n",
    "\n",
    ">**for** episode $k = 1,2,...$ **do**\n",
    ">> sample $M_k \\sim f(\\bullet\\ |\\ H_k)$\n",
    "\n",
    ">> compute policy $\\mu_k$ for $M_k$\n",
    "\n",
    ">> **for** time $t = 1, 2,...$ **do**\n",
    "\n",
    ">>> take action  $a_t$ from $\\mu_k$ \n",
    "\n",
    ">>> observe $r_t$ and $s_{t+1}$\n",
    ">>> update $H_k$\n",
    "\n",
    ">> **end for**\n",
    "\n",
    ">**end for**\n",
    "\n",
    "In our case we will model $M_k$ with two matricies: transition and reward. Transition matrix is sampled from dirichlet distribution. Reward matrix is sampled from normal-gamma distribution.\n",
    "\n",
    "Distributions are updated with bayes rule - see continious distribution section at https://en.wikipedia.org/wiki/Conjugate_prior\n",
    "\n",
    "Article on PSRL - https://arxiv.org/abs/1306.0940"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_normal_gamma(mu, lmbd, alpha, beta):\n",
    "    \"\"\" https://en.wikipedia.org/wiki/Normal-gamma_distribution\n",
    "    \"\"\"\n",
    "    tau = np.random.gamma(alpha, beta)\n",
    "    mu = np.random.normal(mu, 1.0 / np.sqrt(lmbd * tau))\n",
    "    return mu, tau\n",
    "\n",
    "\n",
    "class PsrlAgent:\n",
    "    def __init__(self, n_states, n_actions, horizon=10):\n",
    "        self._n_states = n_states\n",
    "        self._n_actions = n_actions\n",
    "        self._horizon = horizon\n",
    "\n",
    "        # params for transition sampling - Dirichlet distribution\n",
    "        self._transition_counts = np.zeros(\n",
    "            (n_states, n_states, n_actions)) + 1.0\n",
    "\n",
    "        # params for reward sampling - Normal-gamma distribution\n",
    "        self._mu_matrix = np.zeros((n_states, n_actions)) + 1.0\n",
    "        self._state_action_counts = np.zeros(\n",
    "            (n_states, n_actions)) + 1.0  # lambda\n",
    "\n",
    "        self._alpha_matrix = np.zeros((n_states, n_actions)) + 1.0\n",
    "        self._beta_matrix = np.zeros((n_states, n_actions)) + 1.0\n",
    "\n",
    "    def _value_iteration(self, transitions, rewards):\n",
    "        # YOU CODE HERE\n",
    "        state_values = < Find action values with value iteration >\n",
    "        return state_values\n",
    "\n",
    "    def start_episode(self):\n",
    "        # sample new  mdp\n",
    "        self._sampled_transitions = np.apply_along_axis(\n",
    "            np.random.dirichlet, 1, self._transition_counts)\n",
    "\n",
    "        sampled_reward_mus, sampled_reward_stds = sample_normal_gamma(\n",
    "            self._mu_matrix,\n",
    "            self._state_action_counts,\n",
    "            self._alpha_matrix,\n",
    "            self._beta_matrix\n",
    "        )\n",
    "\n",
    "        self._sampled_rewards = sampled_reward_mus\n",
    "        self._current_value_function = self._value_iteration(\n",
    "            self._sampled_transitions, self._sampled_rewards)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        return np.argmax(self._sampled_rewards[state] +\n",
    "                         self._current_value_function.dot(self._sampled_transitions[state]))\n",
    "\n",
    "    def update(self, state, action, reward, next_state):\n",
    "        # YOUR CODE HERE\n",
    "        # update rules - https://en.wikipedia.org/wiki/Conjugate_prior\n",
    "\n",
    "    def get_q_matrix(self):\n",
    "        return self._sampled_rewards + self._current_value_function.dot(self._sampled_transitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "moving_average = lambda x, **kw: DataFrame(\n",
    "    {'x': np.asarray(x)}).x.ewm(**kw).mean().values\n",
    "\n",
    "horizon = 20\n",
    "env = RiverSwimEnv(max_steps=horizon)\n",
    "agent = PsrlAgent(env.n_states, env.n_actions, horizon=horizon)\n",
    "rews = train_mdp_agent(agent, env, 1000)\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.plot(moving_average(np.array(rews), alpha=0.1))\n",
    "\n",
    "plt.xlabel(\"Episode count\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_policy(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus 3.2 Bootstrapped DQN (10 points)\n",
    "\n",
    "Implement Bootstrapped DQN algorithm and compare it's performance with ordinary DQN on BeamRider Atari game. Links:\n",
    "- https://arxiv.org/abs/1602.04621"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
